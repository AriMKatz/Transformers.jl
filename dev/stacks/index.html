<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Stacks · Transformers.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>Transformers.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Home</a></li><li><a class="toctext" href="../tutorial/">Tutorial</a></li><li><a class="toctext" href="../basic/">Basic</a></li><li class="current"><a class="toctext" href>Stacks</a><ul class="internal"><li><a class="toctext" href="#The-Stack-NNTopo-DSL-1">The Stack NNTopo DSL</a></li><li><a class="toctext" href="#NNTopo-Syntax-1">NNTopo Syntax</a></li><li><a class="toctext" href="#Stack-1">Stack</a></li></ul></li><li><a class="toctext" href="../pretrain/">Pretrain</a></li><li><span class="toctext">Models</span><ul><li><a class="toctext" href="../gpt/">GPT</a></li><li><a class="toctext" href="../bert/">BERT</a></li></ul></li><li><a class="toctext" href="../datasets/">Datasets</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Stacks</a></li></ul><a class="edit-page" href="https://github.com/chengchingwen/Transformers.jl/blob/master/docs/src/stacks.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Stacks</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Transformers.Stacks-1" href="#Transformers.Stacks-1">Transformers.Stacks</a></h1><p>Helper struct and DSL for stacking functions/layers.</p><p>Take a simple encoder-decoder model construction of machine translation task. With <code>Transformers.jl</code> we can easily define/stack the models. </p><pre><code class="language-julia">using Transformers
using Transformers.Basic

encoder = Stack(
    @nntopo(e → pe:(e, pe) → x → x → $N),
    PositionEmbedding(512),
    (e, pe) -&gt; e .+ pe,
    Dropout(0.1),
    [Transformer(512, 8, 64, 2048) for i = 1:N]...
)

decoder = Stack(
    @nntopo((e, m, mask):e → pe:(e, pe) → t → (t:(t, m, mask) → t:(t, m, mask)) → $N:t → c),
    PositionEmbedding(512),
    (e, pe) -&gt; e .+ pe,
    Dropout(0.1),
    [TransformerDecoder(512, 8, 64, 2048) for i = 1:N]...,
    Positionwise(Dense(512, length(labels)), logsoftmax)
)

function loss(src, trg, src_mask, trg_mask)
    label = onehot(vocab, trg)

    src = embedding(src)
    trg = embedding(trg)

    mask = getmask(src_mask, trg_mask)

    enc = encoder(src)
    dec = decoder(trg, enc, mask)

    loss = logkldivergence(label, dec[:, 1:end-1, :], trg_mask[:, 1:end-1, :])
end</code></pre><p>See <code>example</code> folder for the complete example.</p><h2><a class="nav-anchor" id="The-Stack-NNTopo-DSL-1" href="#The-Stack-NNTopo-DSL-1">The Stack NNTopo DSL</a></h2><p>Since the <code>TransformerDecoder</code> require more than one input, it&#39;s not convenient to use with <code>Chain</code>. Therefore, we implement a very simple  DSL(Domain Specific Language) to handle the function structure. You can use the <code>@nntopo</code> macro to define the structure then call the function  with the given model.</p><h2><a class="nav-anchor" id="NNTopo-Syntax-1" href="#NNTopo-Syntax-1">NNTopo Syntax</a></h2><p>we call the DSL NNTopo for &quot;Neural Network Topology&quot;, but actually it is just used to define where the input &amp; output should be in a sequence of  function, or the complex version of the <code>|&gt;</code> function in Julia.</p><h3><a class="nav-anchor" id="&quot;Chain&quot;-the-functions-1" href="#&quot;Chain&quot;-the-functions-1">&quot;Chain&quot; the functions</a></h3><p>For example:</p><pre><code class="language-julia">y = h(f(g(x))) #a chain of function call

# or 
a = g(x)
b = f(a)
y = h(b)

# is equivalent to 
topo = @nntopo x =&gt; a =&gt; b =&gt; y # first we define the topology/architecture
y = topo((g, f, h), x) #then call on the given functions</code></pre><p>each <code>=&gt;</code> is a function call, left hand side is the input argument and right hand side is the output name.</p><h3><a class="nav-anchor" id="Loop-unrolling-1" href="#Loop-unrolling-1">Loop unrolling</a></h3><p>you can also unroll a loop:</p><pre><code class="language-julia">y = g(f(f(f(f(x)))))

# or 
tmp = x
for i = 1:4
  tmp = f(tmp)
end
y = g(tmp)

# is equivalent to 
topo = @nntopo x =&gt; 4 =&gt; y
y = topo((f,f,f,f, g), x) # f can also be different</code></pre><h3><a class="nav-anchor" id="Multiple-argument-and-jump-connection-1" href="#Multiple-argument-and-jump-connection-1">Multiple argument &amp; jump connection</a></h3><p>As we metioned above, the original intention was to handle the case that we have more than one input &amp; output. So, we can do this with the following syntax: </p><pre><code class="language-julia"># a complex structure
# x1 to x4 in the given inputs
t = f(x1, x2)
z1, z2 = g(t, x3)
w = h(x4, z1)
y = k(x2, z2, w)

# is equivalent to 
topo = @nntopo (x1, x2, x3, x4):(x1, x2) =&gt; t:(t, x3) =&gt; (z1, z2):(x4, z1) =&gt; w:(x2, z2, w) =&gt; y
y = topo((f, g, h, k), x1, x2, x3, x4)

# you can also see the function with `print_topo` function
using Transformers.Basic: print_topo

print_topo(topo; models=(f, g, h, k))
# 
# NNTopo{&quot;(x1, x2, x3, x4):(x1, x2) =&gt; (t:(t, x3) =&gt; ((z1, z2):(x4, z1) =&gt; (w:(x2, z2, w) =&gt; y)))&quot;}
# topo_func(model, x1, x2, x3, x4)
#         t = f(x1, x2)
#         (z1, z2) = g(t, x3)
#         w = h(x4, z1)
#         y = k(x2, z2, w)
#         y
# end</code></pre><h3><a class="nav-anchor" id="Specify-the-variables-you-want-1" href="#Specify-the-variables-you-want-1">Specify the variables you want</a></h3><p>Notice that we use a <code>:</code> to seperate the input/output variables name for each function call, if the <code>:</code> is not present, we will by default assume  the output variables are all the inputs of the next function call. i.e. <code>x =&gt; (t1, t2) =&gt; y</code> is equal to <code>x =&gt; (t1, t2):(t1, t2) =&gt; y</code>. </p><p>We can also return multiple variables, so the complete syntax can be viewed as:</p><pre><code class="language-none">    (input arguments):(function1 inputs) =&gt; (function1 outputs):(function2 inputs):(function2 outputs) =&gt; .... =&gt; (function_n outputs):(return variables)</code></pre><h3><a class="nav-anchor" id="Interpolation-1" href="#Interpolation-1">Interpolation</a></h3><p>we also support interpolation, so you can use a variable to hold a substructure or the unroll number. But <strong>notice</strong> that the  interpolation variable should always be at the top level of the module since we can only get that value with <code>eval</code>. To use  interpolte local variables, use <code>@nntopo_str &quot;topo_pattern&quot;</code> instead.</p><pre><code class="language-julia">N = 3
topo = @nntopo((e, m, mask):e → pe:(e, pe) → t → (t:(t, m, mask) → t:(t, m, mask)) → $N:t → c)

# or
# topo = @nntopo_str &quot;(e, m, mask):e → pe:(e, pe) → t → (t:(t, m, mask) → t:(t, m, mask)) → $N:t → c&quot;

print_topo(topo)
# 
# NNTopo{&quot;(e, m, mask):e → (pe:(e, pe) → (t → ((t:(t, m, mask) → t:(t, m, mask)) → (3:t → c))))&quot;}
# topo_func(model, e, m, mask)
#         pe = model[1](e)
#         t = model[2](e, pe)
#         t = model[3](t)
#         t = model[4](t, m, mask)
#         t = model[5](t, m, mask)
#         t = model[6](t, m, mask)
#         c = model[7](t)
#         c
# end</code></pre><h3><a class="nav-anchor" id="Nested-Structure-1" href="#Nested-Structure-1">Nested Structure</a></h3><p>you can also use the <code>()</code> to create a nested structure for the unroll.</p><pre><code class="language-julia">topo = @nntopo x =&gt; ((y =&gt; z =&gt; t) =&gt; 3 =&gt; w) =&gt; 2
print_topo(topo)
# 
# NNTopo{&quot;x =&gt; (((y =&gt; (z =&gt; t)) =&gt; (3 =&gt; w)) =&gt; 2)&quot;}
# topo_func(model, x)
#         y = model[1](x)
#         z = model[2](y)
#         t = model[3](z)
#         z = model[4](t)
#         t = model[5](z)
#         z = model[6](t)
#         t = model[7](z)
#         w = model[8](t)
#         z = model[9](w)
#         t = model[10](z)
#         z = model[11](t)
#         t = model[12](z)
#         z = model[13](t)
#         t = model[14](z)
#         w = model[15](t)
#         w
# end</code></pre><h3><a class="nav-anchor" id="Collect-Variables-1" href="#Collect-Variables-1">Collect Variables</a></h3><p>you can also collect some variables that you are interested in with <code>&#39;</code> on that variable. For example:</p><pre><code class="language-julia">julia&gt; @nntopo x =&gt; y&#39; =&gt; 3 =&gt; z
NNTopo{&quot;x =&gt; (y&#39; =&gt; (3 =&gt; z))&quot;}
topo_func(model, x)
        y = model[1](x)
        %1 = y
        y = model[2](y)
        %2 = y
        y = model[3](y)
        %3 = y
        y = model[4](y)
        %4 = y
        z = model[5](y)
        (z, (%1, %2, %3, %4))
end

julia&gt; @nntopo (x,y) =&gt; (a,b,c,d&#39;) =&gt; (w&#39;,r&#39;,y) =&gt; (m,n)&#39; =&gt; z
NNTopo{&quot;(x, y) =&gt; ((a, b, c, d&#39;) =&gt; ((w&#39;, r&#39;, y) =&gt; (((m, n))&#39; =&gt; z)))&quot;}
topo_func(model, x, y)
        (a, b, c, d) = model[1](x, y)
        %1 = d
        (w, r, y) = model[2](a, b, c, d)
        %2 = (w, r)
        (m, n) = model[3](w, r, y)
        %3 = (m, n)
        z = model[4](m, n)
        (z, (%1, %2, %3))
end</code></pre><h2><a class="nav-anchor" id="Stack-1" href="#Stack-1">Stack</a></h2><p>With the NNTopo DSL, now we can simple use the NNTopo with our Stack type, which is also like the <code>Chain</code> but we also need to pass in the  <code>topo</code> for the architecture. You can check the actual function call with <code>show_stackfunc</code>.</p><pre><code class="language-julia">#The Decoder Example in Attention is All you need
using Transformers.Stacks
Stack(
@nntopo((e, m, mask):e → pe:(e, pe) → t → (t:(t, m, mask) → t:(t, m, mask)) → $N:t → c),
PositionEmbedding(512),
(e, pe) -&gt; e .+ pe,
Dropout(0.1),
[TransformerDecoder(512, 8, 64, 2048) for i = 1:N]...,
Positionwise(Dense(512, length(labels)), logsoftmax)
)

julia&gt; show_stackfunc(s)
topo_func(model, e, m, mask)
        pe = PositionEmbedding(512)(e)
        t = getfield(Main, Symbol(&quot;##23#25&quot;))()(e, pe)
        t = Dropout{Float64}(0.1, true)(t)
        t = TransformerDecoder(head=8, head_size=64, pwffn_size=2048, size=512, dropout=0.1)(t, m, mask)
        t = TransformerDecoder(head=8, head_size=64, pwffn_size=2048, size=512, dropout=0.1)(t, m, mask)
        t = TransformerDecoder(head=8, head_size=64, pwffn_size=2048, size=512, dropout=0.1)(t, m, mask)
        c = Positionwise{Tuple{Dense{typeof(identity),TrackedArray{…,Array{Float32,2}},TrackedArray{…,Array{Float32,1}}},typeof(logsoftmax)}}((Dense(512, 12), NNlib.logsoftmax))(t)
        c
end</code></pre><footer><hr/><a class="previous" href="../basic/"><span class="direction">Previous</span><span class="title">Basic</span></a><a class="next" href="../pretrain/"><span class="direction">Next</span><span class="title">Pretrain</span></a></footer></article></body></html>
