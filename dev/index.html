<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · Transformers.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>Transformers.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href>Home</a><ul class="internal"><li><a class="toctext" href="#Installation-1">Installation</a></li><li><a class="toctext" href="#Implemented-model-1">Implemented model</a></li><li><a class="toctext" href="#Example-1">Example</a></li><li class="toplevel"><a class="toctext" href="#Module-Hierarchy-1">Module Hierarchy</a></li><li><a class="toctext" href="#Outline-1">Outline</a></li></ul></li><li><a class="toctext" href="basic/">Basic</a></li><li><a class="toctext" href="stacks/">Stacks</a></li><li><a class="toctext" href="pretrain/">Pretrain</a></li><li><span class="toctext">Models</span><ul><li><a class="toctext" href="gpt/">GPT</a></li><li><a class="toctext" href="bert/">BERT</a></li></ul></li><li><a class="toctext" href="datasets/">Datasets</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Home</a></li></ul><a class="edit-page" href="https://github.com/chengchingwen/Transformers.jl/blob/master/docs/src/index.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Home</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Transformers.jl-1" href="#Transformers.jl-1">Transformers.jl</a></h1><p><em>Julia implementation of Transformers models</em></p><p>This is the documentation of <code>Transformers</code>: The Julia solution for using Transformer models based on <a href="https://fluxml.ai/">Flux.jl</a></p><h2><a class="nav-anchor" id="Installation-1" href="#Installation-1">Installation</a></h2><p>In the Julia REPL:</p><pre><code class="language-jl">julia&gt; ]add Transformers</code></pre><p>For using GPU, install &amp; build:</p><pre><code class="language-jl">julia&gt; ]add CuArrays; build</code></pre><h2><a class="nav-anchor" id="Implemented-model-1" href="#Implemented-model-1">Implemented model</a></h2><p>You can find the code in <code>example</code> folder.</p><ul><li><a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a></li><li><a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">Improving Language Understanding by Generative Pre-Training</a></li><li><a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li></ul><h2><a class="nav-anchor" id="Example-1" href="#Example-1">Example</a></h2><p>Take a simple encoder-decoder model construction of machine translation task. With <code>Transformers.jl</code> we can easily define/stack the models. </p><pre><code class="language-julia">using Transformers
using Transformers.Basic

encoder = Stack(
    @nntopo(e → pe:(e, pe) → x → x → $N),
    PositionEmbedding(512),
    (e, pe) -&gt; e .+ pe,
    Dropout(0.1),
    [Transformer(512, 8, 64, 2048) for i = 1:N]...
)

decoder = Stack(
    @nntopo((e, m, mask):e → pe:(e, pe) → t → (t:(t, m, mask) → t:(t, m, mask)) → $N:t → c),
    PositionEmbedding(512),
    (e, pe) -&gt; e .+ pe,
    Dropout(0.1),
    [TransformerDecoder(512, 8, 64, 2048) for i = 1:N]...,
    Positionwise(Dense(512, length(labels)), logsoftmax)
)

function loss(src, trg, src_mask, trg_mask)
    label = onehot(vocab, trg)

    src = embedding(src)
    trg = embedding(trg)

    mask = getmask(src_mask, trg_mask)

    enc = encoder(src)
    dec = decoder(trg, enc, mask)

    loss = logkldivergence(label, dec[:, 1:end-1, :], trg_mask[:, 1:end-1, :])
end</code></pre><h1><a class="nav-anchor" id="Module-Hierarchy-1" href="#Module-Hierarchy-1">Module Hierarchy</a></h1><ul><li><a href="basic/">Transformers.Basic</a></li></ul><p>Basic functionality of Transformers.jl, provide the Transformer encoder/decoder implementation and other convenient function.</p><ul><li><a href="pretrain/">Transformers.Pretrain</a></li></ul><p>Functions for download and loading pretrain models.</p><ul><li><a href="stacks/">Transformers.Stacks</a></li></ul><p>Helper struct and DSL for stacking functions/layers.</p><ul><li><a href="datasets/">Transformers.Datasets</a></li></ul><p>Functions for loading some common Datasets</p><ul><li><a href="gpt/">Transformers.GenerativePreTrain</a></li></ul><p>Implementation of gpt-1 model</p><ul><li><a href="bert/">Transformers.BidirectionalEncoder</a></li></ul><p>Implementation of BERT model</p><h2><a class="nav-anchor" id="Outline-1" href="#Outline-1">Outline</a></h2><ul><li><a href="basic/#Transformers.Basic-1">Transformers.Basic</a></li><ul><li><a href="basic/#Transformer-1">Transformer</a></li><li><a href="basic/#Positionwise-1">Positionwise</a></li><li><a href="basic/#PositionEmbedding-1">PositionEmbedding</a></li><li><a href="basic/#API-Reference-1">API Reference</a></li></ul><li><a href="stacks/#Transformers.Stacks-1">Transformers.Stacks</a></li><ul><li><a href="stacks/#The-Stack-NNTopo-DSL-1">The Stack NNTopo DSL</a></li><ul><li><a href="stacks/#NNTopo-Syntax-1">NNTopo Syntax</a></li><li><a href="stacks/#Stack-1">Stack</a></li></ul></ul><li><a href="pretrain/#Transformers.Pretrain-1">Transformers.Pretrain</a></li><ul><li><a href="pretrain/#using-Pretrains-1">using Pretrains</a></li><li><a href="pretrain/#API-reference-1">API reference</a></li></ul><li><a href="gpt/#Transformers.GenerativePreTrain-1">Transformers.GenerativePreTrain</a></li><ul><li><a href="gpt/#API-reference-1">API reference</a></li></ul><li><a href="bert/#Transformers.BidirectionalEncoder-1">Transformers.BidirectionalEncoder</a></li><ul><li><a href="bert/#API-reference-1">API reference</a></li></ul><li><a href="datasets/#Transformers.Datasets-(not-complete)-1">Transformers.Datasets (not complete)</a></li><ul><li><a href="datasets/#Provide-datasets-1">Provide datasets</a></li><li><a href="datasets/#API-reference-1">API reference</a></li></ul></ul><footer><hr/><a class="next" href="basic/"><span class="direction">Next</span><span class="title">Basic</span></a></footer></article></body></html>
