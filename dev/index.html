<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · Transformers.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href><img class="logo" src="assets/logo.svg" alt="Transformers.jl logo"/></a><h1>Transformers.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href>Home</a><ul class="internal"><li><a class="toctext" href="#Installation-1">Installation</a></li><li><a class="toctext" href="#Implemented-model-1">Implemented model</a></li><li><a class="toctext" href="#Example-1">Example</a></li><li class="toplevel"><a class="toctext" href="#Module-Hierarchy-1">Module Hierarchy</a></li><li><a class="toctext" href="#Outline-1">Outline</a></li></ul></li><li><a class="toctext" href="tutorial/">Tutorial</a></li><li><a class="toctext" href="basic/">Basic</a></li><li><a class="toctext" href="stacks/">Stacks</a></li><li><a class="toctext" href="pretrain/">Pretrain</a></li><li><span class="toctext">Models</span><ul><li><a class="toctext" href="gpt/">GPT</a></li><li><a class="toctext" href="bert/">BERT</a></li></ul></li><li><a class="toctext" href="datasets/">Datasets</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Home</a></li></ul><a class="edit-page" href="https://github.com/chengchingwen/Transformers.jl/blob/master/docs/src/index.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Home</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Transformers.jl-1" href="#Transformers.jl-1">Transformers.jl</a></h1><p><em>Julia implementation of Transformers models</em></p><p>This is the documentation of <code>Transformers</code>: The Julia solution for using Transformer models based on <a href="https://fluxml.ai/">Flux.jl</a></p><h2><a class="nav-anchor" id="Installation-1" href="#Installation-1">Installation</a></h2><p>In the Julia REPL:</p><pre><code class="language-jl">julia&gt; ]add Transformers</code></pre><p>For using GPU, install &amp; build:</p><pre><code class="language-jl">julia&gt; ]add CuArrays; build</code></pre><h2><a class="nav-anchor" id="Implemented-model-1" href="#Implemented-model-1">Implemented model</a></h2><p>You can find the code in <code>example</code> folder.</p><ul><li><a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a></li><li><a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">Improving Language Understanding by Generative Pre-Training</a></li><li><a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li></ul><h2><a class="nav-anchor" id="Example-1" href="#Example-1">Example</a></h2><p>Using pretrained Bert with <code>Transformers.jl</code>.</p><pre><code class="language-julia">using Transformers
using Transformers.Basic
using Transformers.Pretrain

ENV[&quot;DATADEPS_ALWAYS_ACCEPT&quot;] = true

bert_model, wordpiece, tokenizer = pretrain&quot;bert-uncased_L-12_H-768_A-12&quot;
vocab = Vocabulary(wordpiece)

text1 = &quot;Peter Piper picked a peck of pickled peppers&quot; |&gt; tokenizer |&gt; wordpiece
text2 = &quot;Fuzzy Wuzzy was a bear&quot; |&gt; tokenizer |&gt; wordpiece

text = [&quot;[CLS]&quot;; text1; &quot;[SEP]&quot;; text2; &quot;[SEP]&quot;]
@assert text == [
    &quot;[CLS]&quot;, &quot;peter&quot;, &quot;piper&quot;, &quot;picked&quot;, &quot;a&quot;, &quot;peck&quot;, &quot;of&quot;, &quot;pick&quot;, &quot;##led&quot;, &quot;peppers&quot;, &quot;[SEP]&quot;, 
    &quot;fuzzy&quot;, &quot;wu&quot;, &quot;##zzy&quot;,  &quot;was&quot;, &quot;a&quot;, &quot;bear&quot;, &quot;[SEP]&quot;
]

token_indices = vocab(text)
segment_indices = [fill(1, length(text1)+2); fill(2, length(text2)+1)]

sample = (tok = token_indices, segment = segment_indices)

bert_embedding = sample |&gt; bert_model.embed
feature_tensors = bert_embedding |&gt; bert_model.transformers</code></pre><h1><a class="nav-anchor" id="Module-Hierarchy-1" href="#Module-Hierarchy-1">Module Hierarchy</a></h1><ul><li><a href="basic/">Transformers.Basic</a></li></ul><p>Basic functionality of Transformers.jl, provide the Transformer encoder/decoder implementation and other convenient function.</p><ul><li><a href="pretrain/">Transformers.Pretrain</a></li></ul><p>Functions for download and loading pretrain models.</p><ul><li><a href="stacks/">Transformers.Stacks</a></li></ul><p>Helper struct and DSL for stacking functions/layers.</p><ul><li><a href="datasets/">Transformers.Datasets</a></li></ul><p>Functions for loading some common Datasets</p><ul><li><a href="gpt/">Transformers.GenerativePreTrain</a></li></ul><p>Implementation of gpt-1 model</p><ul><li><a href="bert/">Transformers.BidirectionalEncoder</a></li></ul><p>Implementation of BERT model</p><h2><a class="nav-anchor" id="Outline-1" href="#Outline-1">Outline</a></h2><ul><li><a href="tutorial/#Tutorial-1">Tutorial</a></li><ul><li><a href="tutorial/#Transformer-model-1">Transformer model</a></li><ul><li><a href="tutorial/#Multi-Head-Attention-1">Multi-Head Attention</a></li><li><a href="tutorial/#Positional-Embedding-1">Positional Embedding</a></li></ul><li><a href="tutorial/#Transformers.jl-1">Transformers.jl</a></li><ul><li><a href="tutorial/#Example-1">Example</a></li><li><a href="tutorial/#Copy-task-1">Copy task</a></li><li><a href="tutorial/#Defining-the-model-1">Defining the model</a></li><li><a href="tutorial/#define-the-loss-and-training-loop-1">define the loss and training loop</a></li><li><a href="tutorial/#Test-our-model-1">Test our model</a></li></ul></ul><li><a href="basic/#Transformers.Basic-1">Transformers.Basic</a></li><ul><li><a href="basic/#Transformer-1">Transformer</a></li><li><a href="basic/#Positionwise-1">Positionwise</a></li><li><a href="basic/#PositionEmbedding-1">PositionEmbedding</a></li><li><a href="basic/#API-Reference-1">API Reference</a></li></ul><li><a href="stacks/#Transformers.Stacks-1">Transformers.Stacks</a></li><ul><li><a href="stacks/#The-Stack-NNTopo-DSL-1">The Stack NNTopo DSL</a></li><li><a href="stacks/#NNTopo-Syntax-1">NNTopo Syntax</a></li><ul><li><a href="stacks/#&quot;Chain&quot;-the-functions-1">&quot;Chain&quot; the functions</a></li><li><a href="stacks/#Loop-unrolling-1">Loop unrolling</a></li><li><a href="stacks/#Multiple-argument-and-jump-connection-1">Multiple argument &amp; jump connection</a></li><li><a href="stacks/#Specify-the-variables-you-want-1">Specify the variables you want</a></li><li><a href="stacks/#Interpolation-1">Interpolation</a></li><li><a href="stacks/#Nested-Structure-1">Nested Structure</a></li><li><a href="stacks/#Collect-Variables-1">Collect Variables</a></li></ul><li><a href="stacks/#Stack-1">Stack</a></li></ul><li><a href="pretrain/#Transformers.Pretrain-1">Transformers.Pretrain</a></li><ul><li><a href="pretrain/#using-Pretrains-1">using Pretrains</a></li><li><a href="pretrain/#API-reference-1">API reference</a></li></ul><li><a href="gpt/#Transformers.GenerativePreTrain-1">Transformers.GenerativePreTrain</a></li><ul><li><a href="gpt/#API-reference-1">API reference</a></li></ul><li><a href="bert/#Transformers.BidirectionalEncoder-1">Transformers.BidirectionalEncoder</a></li><ul><li><a href="bert/#Get-Pretrain-1">Get Pretrain</a></li><li><a href="bert/#Finetuning-1">Finetuning</a></li><li><a href="bert/#API-reference-1">API reference</a></li></ul><li><a href="datasets/#Transformers.Datasets-(not-complete)-1">Transformers.Datasets (not complete)</a></li><ul><li><a href="datasets/#Provide-datasets-1">Provide datasets</a></li><li><a href="datasets/#example-1">example</a></li><li><a href="datasets/#API-reference-1">API reference</a></li></ul></ul><footer><hr/><a class="next" href="tutorial/"><span class="direction">Next</span><span class="title">Tutorial</span></a></footer></article></body></html>
