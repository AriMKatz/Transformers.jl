* Transformers.jl
Julia implementation of NLP models, that based on google [[https://arxiv.org/abs/1706.03762][transformer]], with [[https://github.com/FluxML/Flux.jl][Flux.jl]].

* implemented model
+ [[https://arxiv.org/abs/1706.03762][Attention is all you need]]
+ [[https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf][Improving Language Understanding by Generative Pre-Training]]

* Known issue
There are some hack in the implementation, will be remove once these packages update.
+ some hack to make gpu work
  + =permutedims_hack=: move gpu array back to cpu to do permutedims than move to gpu, 
    due to performance issue on permutedims on =CuArray=.
  + =broadcast_add=: =.+= casue =CUDA too many resource requested error= on =Flux.back!=, 
    use =reshape= to avoid that error
+ =batchedmul=: Currently =Flux.jl= doesn't have a batched matrix multiply function, 
  so I implement one.
  + =batched_gemm!=: borrow the implemetation from =BatchedRoutines.jl=
+ =gelu=: cpu & gpu version of =gelu=, can be remove when =NNlib.jl= & =CuArrays.jl= has one.

* Roadmap
+ refactor code
+ optimize performance
+ text related util functions
+ better dataset API
+ more datasets
+ openai gpt model
+ google bert model
