* Transformers.jl
Julia implementation of NLP models, that based on google [[https://arxiv.org/abs/1706.03762][transformer]], with [[https://github.com/FluxML/Flux.jl][Flux.jl]].
For using the model, see =example= folder.

Install:
#+BEGIN_EXAMPLE
]add Transformers

#Currently the Dataset need the HTTP#master to download
]add HTTP#master
#+END_EXAMPLE

* implemented model
+ [[https://arxiv.org/abs/1706.03762][Attention is all you need]]
+ [[https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf][Improving Language Understanding by Generative Pre-Training]]

* Issue
Currently the code is really ugly, need refactor, test and docs.

* Roadmap
  - [ ] write docs
  - [ ] write test
  - [ ] refactor code
  - [X] better embedding functions
  - [ ] lazy CuArrays loading
  - [ ] using HTTP to handle dataset download (need HTTP.jl update)
  - [ ] optimize performance
  - [ ] text related util functions
  - [ ] better dataset API
  - [ ] more datasets
  - [X] openai gpt model
  - [ ] openai gpt-2 model
  - [ ] google bert model
